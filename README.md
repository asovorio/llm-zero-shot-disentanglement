# LLM Zero-Shot Disentanglement

## Overview
This repository hosts the code for a state-of-the-art zero-shot large-language-model system on the Ubuntu IRC conversation disentanglement benchmark. We began reimplementing the anonymous ACL submission at https://openreview.net/pdf?id=jUCowqC6Lc, then iterated with improvement such as prompt tuning, strengthening the base models, or a family of ensembles to experiment with. The resulting Direct Assignment (DA) and Best Response (BR) variants surpass the previous state-of-the-art Conversation-level precision/recall of ≈53 (as of November 2025) by exceeding 70 on both metrics, and raising other clustering level metrics such as Adjusted Rand Index from ≈83 to ≈89.

## Highlights
- Prompt-optimized DA and BR pipelines that use structured JSON outputs for reproducibility.
- Support for both single-request (sync) and OpenAI Batch API (async) entrypoints for cheaper experiments. DA use with BatchAPI support is not recommended if the experiment doesn't have enough scale. Latency can be a problem.
- Ensembles that blend DA/BR and auxiliary models using different strategies to experiment with cost reduction and/or specific metric improvements.
- Evaluation suite for clustering and link metrics, plus results analysis scripts (inspection of specific chunk results, sizes of clusters predicted perfectly, over-merging / under-merging bias, etc).

## Repository Layout
- `configs/` – YAML configs for different runs (`default.yaml`, `batch.yaml`, `ensemble.yaml`, …).
- `scripts/` – Top-level entrypoints (`run_*`, `batch_*`, `evaluate.py`, ensembles, data prep, results analysis).
- `src/disentangle/` – Library code for dataset creation and manipulation, API support (for both OpenAI and Anthropic), prompting, method runners, utilities.
- `data/` – Raw HuggingFace exports, processed chunks, model predictions, ensemble outputs. Any type of data / results needed or generated by the models.
- `prompts/` – System/user prompt templates consumed by the runners.

## Set-up
**Environment**  

Create a virtual environment (for best-practices) and run the following command to install all the required libraries:
pip install -r requirements.txt

Set up API credentials:
Add your OPENAI_API_KEY (and optionally ANTHROPIC_API_KEY if you switch providers) to your system's environment variables.
All command-line tools read keys from environment variables.

**Data Preparation**:

The current repo already has all the data ready to run the models but, if you want to rerun the preparation of the data then:

python scripts/data_preparation/prepare_data.py --input data/raw/ubuntu_irc/ubuntu_validation.jsonl --split dev --out data/processed/ubuntu_irc

This command prepares the data for the dev split of the Ubuntu IRC dataset, and writes it in data/processed.

You can then also use scripts/data_preparation/create_subset.py to down-sample for cheaper smoke tests.


**Configuration**

Every runner accepts --config <yaml>. Editing the respective YAML is the supported way to modify datasets, model snapshots, prompt roots, result directories, and evaluation settings, that way no code changes required and there is no hard-coding in code.

configs/default.yaml -> powers synchronous API runs of DA and BR.
configs/batch.yaml  -> mirrors default.yaml but running DA and BR on the Batch API. Adds a batch section for request/output locations.
configs/ensemble.yaml -> powers all the ensembles.

## Running experiments

Running the Base Methods:

**Direct Assignment (DA)**

python scripts/run_direct_assignment.py --config configs/default.yaml --workers 4
Outputs land in data/results/direct_assignment/<split>/predictions.jsonl. Increase --workers to parallelize chunk processing (parallelism is across chunks since they are all indepedent between them; intra-chunk order stays deterministic).

**Best Response (BR)**

python scripts/run_best_response.py --config configs/default.yaml --workers 4
Results are written to data/results/best_response/<split>/predictions.jsonl.

**Batch API Support for DA and BR**

Use the OpenAI Batch API for large DA jobs or any experiment for BR (its latency isn't as affected using the Batch API since only one Batch API call is necessary instead of 50, one per message inside a chunk, for DA.):

python scripts/batch_direct_assignment.py --config configs/batch.yaml

python scripts/batch_best_response.py --config configs/batch.yaml

Each script hydrates batch/in/requests_step_*.jsonl, downloads batch outputs under batch/out/, and assembles predictions.jsonl under paths.results_dir. 
To monitor specific jobs or retrieve status during the execution run this script:

python scripts/query_batch.py --id batch_XXXXXXXX


**Ensembles**

Blend multiple prediction.jsonl sources. 
For example, to run the consensus conversations (intersection) of DA+BR ensemble run:

python scripts/ensembles/ensemble_consensus_clusters.py --config configs/ensemble.yaml

configs/ensemble.yaml lists the constituent prediction files and writes the consensus to cfg.paths.results_dir/consensus/<split>/predictions.jsonl. Swap in alternative prediction.jsonl paths in the YAML to experiment with new mixtures.


**Evaluation**

Score any predictions.jsonl file against the gold chunk labels, using the respective yaml file used for the experiment that generated that predictions.jsonl:

python scripts/evaluate.py --config configs/default.yaml --predictions data/results/direct_assignment/dev/predictions.jsonl

The evaluator computes all clustering/link metrics (nmi, ari, exact_p, etc.) and can be reused for DA, BR, batch, or ensemble outputs.

**Results analysis**

There are several scripts in scripts/results_analysis/ to analyze the results of the models to uncover error patterns, biases in the models, etc.

For instance, to run a script that prints a chunk of 50 messages with the predicted clusterings and the true clusterings:

python scripts/ensembles/diagnose_pred_errors.py
